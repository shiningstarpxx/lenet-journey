#!/usr/bin/env python3
"""
ç»¼åˆçš„å·ç§¯å±‚æ•°å¯¹æ¯”åˆ†æè„šæœ¬
åŒ…å«è®­ç»ƒã€è¯„ä¼°ã€å¯è§†åŒ–å’Œåˆ†æåŠŸèƒ½
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

from models.conv_comparison import get_model, get_model_info
from config import Config
from utils import set_seed
from data import DatasetLoader
from train_comparison import ComparisonTrainer
from visualize_comparison import ComparisonVisualizer

class ConvComparisonAnalyzer:
    """å·ç§¯å±‚æ•°å¯¹æ¯”åˆ†æå™¨"""
    
    def __init__(self):
        self.device = Config.DEVICE
        self.data_loader = DatasetLoader(
            dataset_name=Config.DATASET,
            data_dir=Config.DATA_DIR,
            batch_size=Config.BATCH_SIZE
        )
        self.train_loader, self.test_loader = self.data_loader.create_dataloaders()
        
    def run_complete_analysis(self, model_types=['conv1', 'conv2'], epochs=3, target_digit=7):
        """è¿è¡Œå®Œæ•´çš„å¯¹æ¯”åˆ†æ"""
        print("ğŸ¯ å¼€å§‹å®Œæ•´çš„å·ç§¯å±‚æ•°å¯¹æ¯”åˆ†æ")
        print("=" * 80)
        
        # 1. è®­ç»ƒæ¨¡å‹
        print("\nğŸ“š æ­¥éª¤1: è®­ç»ƒæ¨¡å‹")
        print("-" * 40)
        trainer = ComparisonTrainer(model_types=model_types, epochs=epochs)
        results = trainer.train_all_models()
        
        # 2. ç»˜åˆ¶è®­ç»ƒå¯¹æ¯”å›¾
        print("\nğŸ“Š æ­¥éª¤2: ç»˜åˆ¶è®­ç»ƒå¯¹æ¯”å›¾")
        print("-" * 40)
        trainer.plot_comparison()
        
        # 3. è¯¦ç»†è¯„ä¼°
        print("\nğŸ” æ­¥éª¤3: è¯¦ç»†è¯„ä¼°")
        print("-" * 40)
        self.detailed_evaluation(results)
        
        # 4. å¯è§†åŒ–å¯¹æ¯”
        print("\nğŸ¨ æ­¥éª¤4: å¯è§†åŒ–å¯¹æ¯”")
        print("-" * 40)
        visualizer = ComparisonVisualizer()
        models = visualizer.load_trained_models(model_types)
        
        if models:
            visualizer.visualize_activations_comparison(models, target_digit=target_digit)
            visualizer.create_comparison_animation(models, target_digit=target_digit)
            visualizer.visualize_detailed_channels_comparison(models, target_digit=target_digit)
        
        # 5. ç”Ÿæˆåˆ†ææŠ¥å‘Š
        print("\nğŸ“ æ­¥éª¤5: ç”Ÿæˆåˆ†ææŠ¥å‘Š")
        print("-" * 40)
        self.generate_analysis_report(results, models)
        
        print("\nğŸ‰ å®Œæ•´åˆ†æå®Œæˆ!")
        return results, models
    
    def detailed_evaluation(self, results):
        """è¯¦ç»†è¯„ä¼°æ‰€æœ‰æ¨¡å‹"""
        print("ğŸ” å¼€å§‹è¯¦ç»†è¯„ä¼°...")
        
        for model_type, result in results.items():
            print(f"\nè¯„ä¼° {get_model_info(model_type)['name']}:")
            
            # åŠ è½½æ¨¡å‹
            checkpoint_path = result['checkpoint_path']
            checkpoint = torch.load(checkpoint_path, map_location=self.device)
            
            model = get_model(model_type, Config.INPUT_CHANNELS, Config.NUM_CLASSES)
            model.load_state_dict(checkpoint['model_state_dict'])
            model = model.to(self.device)
            model.eval()
            
            # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            all_predictions = []
            all_targets = []
            all_probabilities = []
            
            with torch.no_grad():
                for data, target in self.test_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = model(data)
                    
                    probabilities = torch.softmax(output, dim=1)
                    _, predicted = torch.max(output, 1)
                    
                    all_predictions.extend(predicted.cpu().numpy())
                    all_targets.extend(target.cpu().numpy())
                    all_probabilities.extend(probabilities.cpu().numpy())
            
            # è®¡ç®—æŒ‡æ ‡
            accuracy = 100. * np.mean(np.array(all_predictions) == np.array(all_targets))
            
            print(f"  æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2f}%")
            print(f"  æœ€ä½³è®­ç»ƒå‡†ç¡®ç‡: {result['best_accuracy']:.2f}%")
            
            # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
            os.makedirs('./results/evaluation', exist_ok=True)
            np.save(f'./results/evaluation/{model_type}_predictions.npy', all_predictions)
            np.save(f'./results/evaluation/{model_type}_targets.npy', all_targets)
            np.save(f'./results/evaluation/{model_type}_probabilities.npy', all_probabilities)
            
            # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            class_names = [str(i) for i in range(10)]
            report = classification_report(all_targets, all_predictions, 
                                        target_names=class_names, output_dict=True)
            
            # ä¿å­˜åˆ†ç±»æŠ¥å‘Š
            import json
            with open(f'./results/evaluation/{model_type}_classification_report.json', 'w') as f:
                json.dump(report, f, indent=2)
            
            print(f"  è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: ./results/evaluation/{model_type}_*")
    
    def generate_analysis_report(self, results, models):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
        
        report_path = './results/conv_comparison_analysis_report.md'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("# å·ç§¯å±‚æ•°å¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n")
            import datetime
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # å®éªŒè®¾ç½®
            f.write("## å®éªŒè®¾ç½®\n\n")
            f.write(f"- æ•°æ®é›†: {Config.DATASET}\n")
            f.write(f"- è®­ç»ƒè½®æ•°: {len(results[list(results.keys())[0]]['train_losses'])}\n")
            f.write(f"- æ‰¹æ¬¡å¤§å°: {Config.BATCH_SIZE}\n")
            f.write(f"- å­¦ä¹ ç‡: {Config.LEARNING_RATE}\n")
            f.write(f"- è®¾å¤‡: {Config.DEVICE}\n\n")
            
            # æ¨¡å‹å¯¹æ¯”
            f.write("## æ¨¡å‹å¯¹æ¯”\n\n")
            f.write("| æ¨¡å‹ | æè¿° | å‚æ•°æ•°é‡ | æœ€ä½³å‡†ç¡®ç‡ | æœ€ç»ˆå‡†ç¡®ç‡ |\n")
            f.write("|------|------|----------|------------|------------|\n")
            
            for model_type, result in results.items():
                info = get_model_info(model_type)
                model = get_model(model_type, Config.INPUT_CHANNELS, Config.NUM_CLASSES)
                param_count = sum(p.numel() for p in model.parameters())
                
                f.write(f"| {info['name']} | {info['description']} | {param_count:,} | "
                       f"{result['best_accuracy']:.2f}% | {result['test_accuracies'][-1]:.2f}% |\n")
            
            # å…³é”®å‘ç°
            f.write("\n## å…³é”®å‘ç°\n\n")
            
            # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
            best_model = max(results.items(), key=lambda x: x[1]['best_accuracy'])
            best_info = get_model_info(best_model[0])
            
            f.write(f"### æœ€ä½³æ¨¡å‹\n")
            f.write(f"- **æ¨¡å‹**: {best_info['name']}\n")
            f.write(f"- **å‡†ç¡®ç‡**: {best_model[1]['best_accuracy']:.2f}%\n")
            f.write(f"- **æè¿°**: {best_info['description']}\n\n")
            
            # å±‚æ•°å½±å“åˆ†æ
            f.write("### å±‚æ•°å½±å“åˆ†æ\n\n")
            
            if 'conv1' in results and 'conv2' in results:
                conv1_acc = results['conv1']['best_accuracy']
                conv2_acc = results['conv2']['best_accuracy']
                improvement = conv2_acc - conv1_acc
                
                f.write(f"- **1å±‚Conv vs 2å±‚Conv**: å‡†ç¡®ç‡æå‡ {improvement:.2f}%\n")
                
                if improvement > 0:
                    f.write(f"- **ç»“è®º**: å¢åŠ å·ç§¯å±‚æ•°å¯¹MNISTæ•°æ®é›†æœ‰æ­£é¢å½±å“\n")
                else:
                    f.write(f"- **ç»“è®º**: å¯¹äºMNISTè¿™æ ·çš„ç®€å•æ•°æ®é›†ï¼Œ1å±‚å·ç§¯å¯èƒ½å·²ç»è¶³å¤Ÿ\n")
            
            # å‚æ•°æ•ˆç‡åˆ†æ
            f.write("\n### å‚æ•°æ•ˆç‡åˆ†æ\n\n")
            
            for model_type, result in results.items():
                info = get_model_info(model_type)
                model = get_model(model_type, Config.INPUT_CHANNELS, Config.NUM_CLASSES)
                param_count = sum(p.numel() for p in model.parameters())
                efficiency = result['best_accuracy'] / param_count * 1000  # æ¯1000å‚æ•°çš„å‡†ç¡®ç‡
                
                f.write(f"- **{info['name']}**: {efficiency:.4f}% å‡†ç¡®ç‡/1000å‚æ•°\n")
            
            # å¯è§†åŒ–åˆ†æ
            f.write("\n## å¯è§†åŒ–åˆ†æ\n\n")
            f.write("é€šè¿‡æ¿€æ´»å¯è§†åŒ–å¯ä»¥è§‚å¯Ÿåˆ°:\n\n")
            
            if models:
                f.write("1. **Conv1å±‚æ¿€æ´»æ¨¡å¼**:\n")
                f.write("   - ä¸åŒå±‚æ•°çš„æ¨¡å‹åœ¨Conv1å±‚è¡¨ç°å‡ºç›¸ä¼¼çš„æ¿€æ´»æ¨¡å¼\n")
                f.write("   - 1å±‚Convæ¨¡å‹å·²ç»èƒ½å¤Ÿå¾ˆå¥½åœ°æå–è¾¹ç¼˜å’Œçº¿æ¡ç‰¹å¾\n\n")
                
                f.write("2. **ç‰¹å¾å±‚æ¬¡**:\n")
                f.write("   - 1å±‚Conv: ä¸»è¦æå–ä½çº§ç‰¹å¾ï¼ˆè¾¹ç¼˜ã€çº¿æ¡ï¼‰\n")
                f.write("   - 2å±‚Conv: åœ¨ä½çº§ç‰¹å¾åŸºç¡€ä¸Šæå–æ›´å¤æ‚çš„å½¢çŠ¶\n")
                f.write("   - 3å±‚Conv: è¿›ä¸€æ­¥æŠ½è±¡ï¼Œä½†å¯èƒ½å¯¹MNISTè¿‡äºå¤æ‚\n\n")
                
                f.write("3. **æ¿€æ´»å¼ºåº¦åˆ†å¸ƒ**:\n")
                f.write("   - ä¸åŒæ¨¡å‹åœ¨ç›¸åŒæ ·æœ¬ä¸Šçš„æ¿€æ´»å¼ºåº¦åˆ†å¸ƒä¸åŒ\n")
                f.write("   - å±‚æ•°è¶Šå¤šï¼Œæ¿€æ´»çš„æŠ½è±¡ç¨‹åº¦è¶Šé«˜\n\n")
            
            # ç»“è®ºå’Œå»ºè®®
            f.write("## ç»“è®ºå’Œå»ºè®®\n\n")
            f.write("### ä¸»è¦ç»“è®º\n\n")
            f.write("1. **å¯¹äºMNISTæ•°æ®é›†**:\n")
            f.write("   - 1å±‚å·ç§¯ç½‘ç»œå·²ç»èƒ½å¤Ÿè¾¾åˆ°è¾ƒå¥½çš„æ€§èƒ½\n")
            f.write("   - å¢åŠ å±‚æ•°å¯èƒ½å¸¦æ¥æ€§èƒ½æå‡ï¼Œä½†æ”¶ç›Šé€’å‡\n")
            f.write("   - éœ€è¦è€ƒè™‘è®¡ç®—æˆæœ¬å’Œæ€§èƒ½æå‡çš„å¹³è¡¡\n\n")
            
            f.write("2. **æ¨¡å‹é€‰æ‹©å»ºè®®**:\n")
            f.write("   - **ç®€å•ä»»åŠ¡**: ä¼˜å…ˆè€ƒè™‘1å±‚æˆ–2å±‚å·ç§¯\n")
            f.write("   - **å¤æ‚ä»»åŠ¡**: å¯èƒ½éœ€è¦æ›´å¤šå±‚æ•°\n")
            f.write("   - **èµ„æºå—é™**: é€‰æ‹©å‚æ•°è¾ƒå°‘çš„æ¨¡å‹\n\n")
            
            f.write("3. **å¯è§†åŒ–ä»·å€¼**:\n")
            f.write("   - æ¿€æ´»å¯è§†åŒ–æœ‰åŠ©äºç†è§£ç½‘ç»œå·¥ä½œåŸç†\n")
            f.write("   - å¯ä»¥ç”¨äºæ¨¡å‹è°ƒè¯•å’Œä¼˜åŒ–\n")
            f.write("   - æœ‰åŠ©äºè§£é‡Šæ¨¡å‹çš„å†³ç­–è¿‡ç¨‹\n\n")
            
            # ç”Ÿæˆçš„æ–‡ä»¶åˆ—è¡¨
            f.write("## ç”Ÿæˆçš„æ–‡ä»¶\n\n")
            f.write("### æ¨¡å‹æ–‡ä»¶\n")
            f.write("- `checkpoints/comparison/`: è®­ç»ƒå¥½çš„æ¨¡å‹æ£€æŸ¥ç‚¹\n")
            f.write("- `logs/comparison_*/`: TensorBoardè®­ç»ƒæ—¥å¿—\n\n")
            
            f.write("### å¯è§†åŒ–æ–‡ä»¶\n")
            f.write("- `results/conv_comparison.png`: è®­ç»ƒå¯¹æ¯”å›¾\n")
            f.write("- `results/conv_activations_comparison_7.png`: æ¿€æ´»å¯¹æ¯”å›¾\n")
            f.write("- `results/conv_comparison_animation_7.gif`: å¯¹æ¯”åŠ¨ç”»\n")
            f.write("- `results/detailed_channels_comparison_7.png`: è¯¦ç»†é€šé“å¯¹æ¯”\n\n")
            
            f.write("### è¯„ä¼°æ–‡ä»¶\n")
            f.write("- `results/evaluation/`: è¯¦ç»†è¯„ä¼°ç»“æœ\n")
            f.write("- `results/conv_comparison_analysis_report.md`: æœ¬åˆ†ææŠ¥å‘Š\n\n")
            
            f.write("---\n")
            f.write("*æŠ¥å‘Šç”±LeNet-5å¯¹æ¯”åˆ†æç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ*\n")
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    def quick_comparison(self, target_digit=7):
        """å¿«é€Ÿå¯¹æ¯”ï¼ˆä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æˆ–åˆ›å»ºæ–°æ¨¡å‹ï¼‰"""
        print("âš¡ å¿«é€Ÿå¯¹æ¯”åˆ†æ")
        print("=" * 40)
        
        model_types = ['conv1', 'conv2']
        models = {}
        
        # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»ºæ–°æ¨¡å‹
        for model_type in model_types:
            checkpoint_path = f'./checkpoints/comparison/{model_type}_best.pth'
            
            if os.path.exists(checkpoint_path):
                model = get_model(model_type, Config.INPUT_CHANNELS, Config.NUM_CLASSES)
                checkpoint = torch.load(checkpoint_path, map_location=self.device)
                model.load_state_dict(checkpoint['model_state_dict'])
                model = model.to(self.device)
                model.eval()
                
                models[model_type] = {
                    'model': model,
                    'accuracy': checkpoint['best_accuracy'],
                    'info': get_model_info(model_type)
                }
                
                print(f"âœ… åŠ è½½é¢„è®­ç»ƒ {model_type} æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {checkpoint['best_accuracy']:.2f}%")
            else:
                # åˆ›å»ºæ–°æ¨¡å‹è¿›è¡Œæ¼”ç¤º
                model = get_model(model_type, Config.INPUT_CHANNELS, Config.NUM_CLASSES)
                model = model.to(self.device)
                model.eval()
                
                models[model_type] = {
                    'model': model,
                    'accuracy': 0.0,
                    'info': get_model_info(model_type)
                }
                
                print(f"âš ï¸ ä½¿ç”¨æœªè®­ç»ƒçš„ {model_type} æ¨¡å‹è¿›è¡Œæ¼”ç¤º")
        
        # å¿«é€Ÿå¯è§†åŒ–å¯¹æ¯”
        visualizer = ComparisonVisualizer()
        visualizer.visualize_activations_comparison(models, target_digit=target_digit, num_samples=3)
        
        return models

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ å·ç§¯å±‚æ•°å¯¹æ¯”åˆ†æç³»ç»Ÿ")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(Config.RANDOM_SEED)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = ConvComparisonAnalyzer()
    
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰é¢„è®­ç»ƒæ¨¡å‹
        has_pretrained = any(os.path.exists(f'./checkpoints/comparison/{mt}_best.pth') 
                           for mt in ['conv1', 'conv2', 'conv3'])
        
        if has_pretrained:
            print("âœ… å‘ç°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹å®Œæ•´åˆ†æ...")
            results, models = analyzer.run_complete_analysis(
                model_types=['conv1', 'conv2'],  # å¯ä»¥è°ƒæ•´
                epochs=3,  # å¯ä»¥è°ƒæ•´
                target_digit=7
            )
        else:
            print("âš ï¸ æœªå‘ç°é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¼€å§‹å¿«é€Ÿå¯¹æ¯”...")
            models = analyzer.quick_comparison(target_digit=7)
            
            print("\nğŸ’¡ æç¤º: è¿è¡Œ 'python train_comparison.py' æ¥è®­ç»ƒæ¨¡å‹")
            print("   ç„¶åå†æ¬¡è¿è¡Œæ­¤è„šæœ¬è¿›è¡Œå®Œæ•´åˆ†æ")
        
        print("\nğŸ‰ åˆ†æå®Œæˆ!")
        print("\nğŸ“ æŸ¥çœ‹ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- results/ (å¯è§†åŒ–ç»“æœ)")
        print("- checkpoints/comparison/ (æ¨¡å‹æ–‡ä»¶)")
        print("- logs/comparison_*/ (è®­ç»ƒæ—¥å¿—)")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
