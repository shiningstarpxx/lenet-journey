#!/usr/bin/env python3
"""
ä¸åŒå±‚æ•°å·ç§¯ç½‘ç»œçš„å¯¹æ¯”è®­ç»ƒè„šæœ¬
æ¯”è¾ƒ1å±‚Convã€2å±‚Convå’Œ3å±‚Convçš„æ•ˆæœ
"""

import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['PingFang SC', 'Hiragino Sans GB', 'STHeiti', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

from models.conv_comparison import get_model, get_model_info
from config import Config
from utils import set_seed, load_checkpoint
from data import DatasetLoader

class ComparisonTrainer:
    """å¯¹æ¯”è®­ç»ƒå™¨"""
    
    def __init__(self, model_types=['conv1', 'conv2'], epochs=5):
        self.model_types = model_types
        self.epochs = epochs
        self.device = Config.DEVICE
        self.results = {}
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.data_loader = DatasetLoader(
            dataset_name=Config.DATASET,
            data_dir=Config.DATA_DIR,
            batch_size=Config.BATCH_SIZE
        )
        self.train_loader, self.test_loader = self.data_loader.create_dataloaders()
        
        print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        print(f"  è®­ç»ƒé›†å¤§å°: {len(self.train_loader.dataset)}")
        print(f"  æµ‹è¯•é›†å¤§å°: {len(self.test_loader.dataset)}")
        print(f"  æ‰¹æ¬¡å¤§å°: {Config.BATCH_SIZE}")
        print(f"  è®¾å¤‡: {self.device}")
    
    def train_model(self, model_type):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ {get_model_info(model_type)['name']}")
        print("=" * 60)
        
        # åˆ›å»ºæ¨¡å‹
        input_channels = getattr(self, 'input_channels', Config.INPUT_CHANNELS)
        model = get_model(model_type, input_channels, Config.NUM_CLASSES)
        model = model.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
        optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)
        criterion = nn.CrossEntropyLoss()
        
        # åˆ›å»ºTensorBoard writer
        log_dir = f'./logs/comparison_{model_type}'
        writer = SummaryWriter(log_dir)
        
        # è®­ç»ƒå†å²
        train_losses = []
        train_accuracies = []
        test_losses = []
        test_accuracies = []
        
        best_accuracy = 0.0
        best_model_state = None
        
        for epoch in range(self.epochs):
            print(f"\nEpoch {epoch+1}/{self.epochs}")
            
            # è®­ç»ƒé˜¶æ®µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                train_total += target.size(0)
                train_correct += (predicted == target).sum().item()
                
                if batch_idx % 100 == 0:
                    print(f'  Batch {batch_idx}/{len(self.train_loader)}, '
                          f'Loss: {loss.item():.4f}, '
                          f'Acc: {100. * train_correct / train_total:.2f}%')
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            avg_train_loss = train_loss / len(self.train_loader)
            train_accuracy = 100. * train_correct / train_total
            train_losses.append(avg_train_loss)
            train_accuracies.append(train_accuracy)
            
            # æµ‹è¯•é˜¶æ®µ
            model.eval()
            test_loss = 0.0
            test_correct = 0
            test_total = 0
            
            with torch.no_grad():
                for data, target in self.test_loader:
                    data, target = data.to(self.device), target.to(self.device)
                    output = model(data)
                    loss = criterion(output, target)
                    
                    test_loss += loss.item()
                    _, predicted = torch.max(output.data, 1)
                    test_total += target.size(0)
                    test_correct += (predicted == target).sum().item()
            
            # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
            avg_test_loss = test_loss / len(self.test_loader)
            test_accuracy = 100. * test_correct / test_total
            test_losses.append(avg_test_loss)
            test_accuracies.append(test_accuracy)
            
            # è®°å½•åˆ°TensorBoard
            writer.add_scalar('Loss/Train', avg_train_loss, epoch)
            writer.add_scalar('Loss/Test', avg_test_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_accuracy, epoch)
            writer.add_scalar('Accuracy/Test', test_accuracy, epoch)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if test_accuracy > best_accuracy:
                best_accuracy = test_accuracy
                best_model_state = model.state_dict().copy()
            
            print(f'  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.2f}%')
            print(f'  æµ‹è¯•æŸå¤±: {avg_test_loss:.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.2f}%')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        os.makedirs('./checkpoints/comparison', exist_ok=True)
        checkpoint_path = f'./checkpoints/comparison/{model_type}_best.pth'
        torch.save({
            'model_state_dict': best_model_state,
            'model_type': model_type,
            'best_accuracy': best_accuracy,
            'epochs': self.epochs,
            'train_losses': train_losses,
            'train_accuracies': train_accuracies,
            'test_losses': test_losses,
            'test_accuracies': test_accuracies
        }, checkpoint_path)
        
        print(f"\nâœ… {get_model_info(model_type)['name']} è®­ç»ƒå®Œæˆ!")
        print(f"   æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_accuracy:.2f}%")
        print(f"   æ¨¡å‹å·²ä¿å­˜åˆ°: {checkpoint_path}")
        
        # å…³é—­TensorBoard writer
        writer.close()
        
        # è¿”å›ç»“æœ
        return {
            'model': model,
            'best_accuracy': best_accuracy,
            'train_losses': train_losses,
            'train_accuracies': train_accuracies,
            'test_losses': test_losses,
            'test_accuracies': test_accuracies,
            'checkpoint_path': checkpoint_path
        }
    
    def train_all_models(self):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print("ğŸ¯ å¼€å§‹å¯¹æ¯”è®­ç»ƒä¸åŒå±‚æ•°çš„å·ç§¯ç½‘ç»œ")
        print("=" * 80)
        
        for model_type in self.model_types:
            info = get_model_info(model_type)
            print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
            print(f"   åç§°: {info['name']}")
            print(f"   æè¿°: {info['description']}")
            print(f"   å‚æ•°: {info['params']}")
            print(f"   å·ç§¯å±‚: {info['layers']}")
            
            # è®­ç»ƒæ¨¡å‹
            result = self.train_model(model_type)
            self.results[model_type] = result
        
        return self.results
    
    def plot_comparison(self, save_path=None):
        """ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨"""
        if not self.results:
            print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœå¯ä»¥ç»˜åˆ¶")
            return
        
        print("\nğŸ“Š ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨...")
        
        # åˆ›å»ºå­å›¾
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        colors = ['blue', 'red', 'green', 'orange', 'purple']
        
        for i, (model_type, result) in enumerate(self.results.items()):
            info = get_model_info(model_type)
            color = colors[i % len(colors)]
            epochs = range(1, len(result['train_losses']) + 1)
            
            # è®­ç»ƒå’Œæµ‹è¯•æŸå¤±
            ax1.plot(epochs, result['train_losses'], color=color, linestyle='-', 
                    label=f"{info['name']} (è®­ç»ƒ)", linewidth=2)
            ax1.plot(epochs, result['test_losses'], color=color, linestyle='--', 
                    label=f"{info['name']} (æµ‹è¯•)", linewidth=2)
            
            # è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡
            ax2.plot(epochs, result['train_accuracies'], color=color, linestyle='-', 
                    label=f"{info['name']} (è®­ç»ƒ)", linewidth=2)
            ax2.plot(epochs, result['test_accuracies'], color=color, linestyle='--', 
                    label=f"{info['name']} (æµ‹è¯•)", linewidth=2)
        
        # è®¾ç½®å›¾è¡¨
        ax1.set_title('è®­ç»ƒå’Œæµ‹è¯•æŸå¤±å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Epoch')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        ax2.set_title('è®­ç»ƒå’Œæµ‹è¯•å‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax2.set_xlabel('Epoch')
        ax2.set_ylabel('Accuracy (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # æœ€ç»ˆå‡†ç¡®ç‡å¯¹æ¯”
        model_names = [get_model_info(mt)['name'] for mt in self.results.keys()]
        final_accuracies = [result['test_accuracies'][-1] for result in self.results.values()]
        best_accuracies = [result['best_accuracy'] for result in self.results.values()]
        
        x = np.arange(len(model_names))
        width = 0.35
        
        ax3.bar(x - width/2, final_accuracies, width, label='æœ€ç»ˆå‡†ç¡®ç‡', alpha=0.8)
        ax3.bar(x + width/2, best_accuracies, width, label='æœ€ä½³å‡†ç¡®ç‡', alpha=0.8)
        ax3.set_title('æœ€ç»ˆå‡†ç¡®ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax3.set_xlabel('æ¨¡å‹')
        ax3.set_ylabel('å‡†ç¡®ç‡ (%)')
        ax3.set_xticks(x)
        ax3.set_xticklabels(model_names, rotation=45)
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # å‚æ•°æ•°é‡å¯¹æ¯”
        param_counts = []
        for model_type in self.results.keys():
            model = get_model(model_type, Config.INPUT_CHANNELS, Config.NUM_CLASSES)
            param_count = sum(p.numel() for p in model.parameters())
            param_counts.append(param_count)
        
        ax4.bar(model_names, param_counts, alpha=0.8, color=colors[:len(model_names)])
        ax4.set_title('æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”', fontsize=14, fontweight='bold')
        ax4.set_xlabel('æ¨¡å‹')
        ax4.set_ylabel('å‚æ•°æ•°é‡')
        ax4.set_xticklabels(model_names, rotation=45)
        ax4.grid(True, alpha=0.3)
        
        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
        for i, v in enumerate(param_counts):
            ax4.text(i, v + max(param_counts) * 0.01, f'{v:,}', 
                    ha='center', va='bottom', fontweight='bold')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        if save_path is None:
            os.makedirs('./results', exist_ok=True)
            save_path = './results/conv_comparison.png'
        
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        plt.show()
        
        return fig
    
    def print_summary(self):
        """æ‰“å°è®­ç»ƒæ€»ç»“"""
        if not self.results:
            print("âŒ æ²¡æœ‰è®­ç»ƒç»“æœå¯ä»¥æ€»ç»“")
            return
        
        print("\n" + "=" * 80)
        print("ğŸ“Š è®­ç»ƒç»“æœæ€»ç»“")
        print("=" * 80)
        
        for model_type, result in self.results.items():
            info = get_model_info(model_type)
            print(f"\n{info['name']}:")
            print(f"  æè¿°: {info['description']}")
            print(f"  æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {result['best_accuracy']:.2f}%")
            print(f"  æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {result['test_accuracies'][-1]:.2f}%")
            print(f"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {result['train_accuracies'][-1]:.2f}%")
            print(f"  æ¨¡å‹æ–‡ä»¶: {result['checkpoint_path']}")
        
        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model = max(self.results.items(), key=lambda x: x[1]['best_accuracy'])
        best_info = get_model_info(best_model[0])
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_info['name']}")
        print(f"   æœ€ä½³å‡†ç¡®ç‡: {best_model[1]['best_accuracy']:.2f}%")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¸åŒå±‚æ•°å·ç§¯ç½‘ç»œå¯¹æ¯”è®­ç»ƒ")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    set_seed(Config.RANDOM_SEED)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = ComparisonTrainer(
        model_types=['conv1', 'conv2', 'conv3'],  # å¯ä»¥è°ƒæ•´è¦è®­ç»ƒçš„æ¨¡å‹
        epochs=5  # å¯ä»¥è°ƒæ•´è®­ç»ƒè½®æ•°
    )
    
    try:
        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        results = trainer.train_all_models()
        
        # ç»˜åˆ¶å¯¹æ¯”å›¾è¡¨
        trainer.plot_comparison()
        
        # æ‰“å°æ€»ç»“
        trainer.print_summary()
        
        print("\nğŸ‰ å¯¹æ¯”è®­ç»ƒå®Œæˆ!")
        print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print("- results/conv_comparison.png (å¯¹æ¯”å›¾è¡¨)")
        print("- checkpoints/comparison/ (æ¨¡å‹æ£€æŸ¥ç‚¹)")
        print("- logs/comparison_*/ (TensorBoardæ—¥å¿—)")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
