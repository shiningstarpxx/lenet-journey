"""
å¢å¼ºç‰ˆå¯è§†åŒ–æ¨¡å—
åŒ…å«æ›´ä¸°å¯Œçš„ç½‘ç»œæ¿€æ´»å¯è§†åŒ–å’ŒåŠ¨ç”»åŠŸèƒ½
"""

import os
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation
from matplotlib.gridspec import GridSpec
import seaborn as sns
from PIL import Image, ImageDraw, ImageFont

def visualize_conv_filters(model, layer_name='conv1', save_path=None):
    """å¯è§†åŒ–å·ç§¯å±‚çš„æ»¤æ³¢å™¨"""
    model.eval()
    
    # è·å–æŒ‡å®šå±‚çš„æƒé‡
    if layer_name == 'conv1':
        conv_layer = model.conv1
    elif layer_name == 'conv2':
        conv_layer = model.conv2
    else:
        raise ValueError("åªæ”¯æŒ conv1 å’Œ conv2 å±‚")
    
    # è·å–æ»¤æ³¢å™¨æƒé‡
    filters = conv_layer.weight.data.cpu().numpy()
    
    # è®¡ç®—å­å›¾å¸ƒå±€
    num_filters = filters.shape[0]
    cols = min(6, num_filters)
    rows = (num_filters + cols - 1) // cols
    
    fig, axes = plt.subplots(rows, cols, figsize=(cols*2, rows*2))
    if rows == 1:
        axes = axes.reshape(1, -1)
    
    for i in range(num_filters):
        row = i // cols
        col = i % cols
        
        filter_img = filters[i]
        
        if filter_img.shape[0] == 1:  # å•é€šé“
            axes[row, col].imshow(filter_img[0], cmap='gray')
        else:  # å¤šé€šé“
            # å¯¹äºå¤šé€šé“æ»¤æ³¢å™¨ï¼Œæ˜¾ç¤ºæ¯ä¸ªé€šé“çš„å¹³å‡å€¼
            filter_avg = filter_img.mean(axis=0)
            axes[row, col].imshow(filter_avg, cmap='gray')
        
        axes[row, col].set_title(f'Filter {i+1}')
        axes[row, col].axis('off')
    
    # éšè—å¤šä½™çš„å­å›¾
    for i in range(num_filters, rows * cols):
        row = i // cols
        col = i % cols
        axes[row, col].axis('off')
    
    plt.suptitle(f'{layer_name.upper()} å·ç§¯æ»¤æ³¢å™¨', fontsize=16)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()

def visualize_conv_activations_for_digit(model, test_loader, device, target_digit=7, 
                                       num_samples=6, save_path=None):
    """é’ˆå¯¹ç‰¹å®šæ•°å­—ï¼ˆå¦‚7ï¼‰å¯è§†åŒ–å¤šä¸ªæ ·æœ¬çš„å·ç§¯å±‚æ¿€æ´»"""
    model.eval()
    
    # æ”¶é›†ç›®æ ‡æ•°å­—çš„æ ·æœ¬
    target_samples = []
    target_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            for i, label in enumerate(labels):
                if label.item() == target_digit and len(target_samples) < num_samples:
                    target_samples.append(data[i])
                    target_labels.append(label.item())
            
            if len(target_samples) >= num_samples:
                break
    
    if len(target_samples) == 0:
        print(f"æœªæ‰¾åˆ°æ•°å­— {target_digit} çš„æ ·æœ¬")
        return
    
    # è·å–æ¿€æ´»
    activations = []
    for sample in target_samples:
        sample_batch = sample.unsqueeze(0).to(device)
        if hasattr(model, 'get_activations'):
            acts = model.get_activations(sample_batch)
            activations.append(acts)
    
    # åˆ›å»ºå¯è§†åŒ–
    fig = plt.figure(figsize=(20, 12))
    gs = GridSpec(3, num_samples, figure=fig)
    
    for i, (sample, acts) in enumerate(zip(target_samples, activations)):
        # åŸå§‹å›¾åƒ
        ax_orig = fig.add_subplot(gs[0, i])
        img = sample.squeeze().cpu().numpy()
        ax_orig.imshow(img, cmap='gray')
        ax_orig.set_title(f'æ ·æœ¬ {i+1}\næ•°å­— {target_digit}')
        ax_orig.axis('off')
        
        # Conv1 æ¿€æ´»
        ax_conv1 = fig.add_subplot(gs[1, i])
        conv1_act = acts['conv1'].squeeze().detach().cpu().numpy()
        # æ˜¾ç¤ºæ‰€æœ‰é€šé“çš„å¹³å‡æ¿€æ´»
        conv1_avg = conv1_act.mean(axis=0)
        im1 = ax_conv1.imshow(conv1_avg, cmap='hot')
        ax_conv1.set_title(f'Conv1 æ¿€æ´»\n(6é€šé“å¹³å‡)')
        ax_conv1.axis('off')
        
        # Conv2 æ¿€æ´»
        ax_conv2 = fig.add_subplot(gs[2, i])
        conv2_act = acts['conv2'].squeeze().detach().cpu().numpy()
        # æ˜¾ç¤ºæ‰€æœ‰é€šé“çš„å¹³å‡æ¿€æ´»
        conv2_avg = conv2_act.mean(axis=0)
        im2 = ax_conv2.imshow(conv2_avg, cmap='hot')
        ax_conv2.set_title(f'Conv2 æ¿€æ´»\n(16é€šé“å¹³å‡)')
        ax_conv2.axis('off')
    
    plt.suptitle(f'æ•°å­— {target_digit} çš„å·ç§¯å±‚æ¿€æ´»å¯è§†åŒ–', fontsize=16)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()

def visualize_conv_channels_detailed(model, test_loader, device, target_digit=7, 
                                   sample_idx=0, save_path=None):
    """è¯¦ç»†å¯è§†åŒ–å·ç§¯å±‚çš„æ¯ä¸ªé€šé“"""
    model.eval()
    
    # è·å–ç›®æ ‡æ ·æœ¬
    target_samples = []
    for data, labels in test_loader:
        for i, label in enumerate(labels):
            if label.item() == target_digit:
                target_samples.append(data[i])
                if len(target_samples) > sample_idx:
                    break
        if len(target_samples) > sample_idx:
            break
    
    if len(target_samples) <= sample_idx:
        print(f"æœªæ‰¾åˆ°è¶³å¤Ÿçš„æ•°å­— {target_digit} æ ·æœ¬")
        return
    
    sample = target_samples[sample_idx].unsqueeze(0).to(device)
    
    # è·å–æ¿€æ´»
    with torch.no_grad():
        if hasattr(model, 'get_activations'):
            acts = model.get_activations(sample)
    
    # åˆ›å»ºè¯¦ç»†å¯è§†åŒ–
    fig = plt.figure(figsize=(24, 16))
    
    # åŸå§‹å›¾åƒ
    ax_orig = plt.subplot2grid((4, 8), (0, 0), colspan=2)
    img = sample.squeeze().cpu().numpy()
    ax_orig.imshow(img, cmap='gray')
    ax_orig.set_title(f'åŸå§‹å›¾åƒ\næ•°å­— {target_digit}')
    ax_orig.axis('off')
    
    # Conv1 å„é€šé“
    conv1_act = acts['conv1'].squeeze().detach().cpu().numpy()
    for i in range(6):
        ax = plt.subplot2grid((4, 8), (0, 2+i))
        ax.imshow(conv1_act[i], cmap='hot')
        ax.set_title(f'Conv1-{i+1}')
        ax.axis('off')
    
    # Conv2 å„é€šé“
    conv2_act = acts['conv2'].squeeze().detach().cpu().numpy()
    for i in range(16):
        row = 1 + i // 8
        col = i % 8
        ax = plt.subplot2grid((4, 8), (row, col))
        ax.imshow(conv2_act[i], cmap='hot')
        ax.set_title(f'Conv2-{i+1}', fontsize=8)
        ax.axis('off')
    
    plt.suptitle(f'æ•°å­— {target_digit} çš„è¯¦ç»†å·ç§¯é€šé“æ¿€æ´»', fontsize=16)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()

def create_activation_animation_enhanced(model, test_loader, device, class_names, 
                                       num_samples=5, save_path=None):
    """å¢å¼ºç‰ˆæ¿€æ´»åŠ¨ç”»ï¼Œå±•ç¤ºæ›´è¯¦ç»†çš„è®¡ç®—è¿‡ç¨‹"""
    model.eval()
    
    # è·å–æ ·æœ¬
    data_iter = iter(test_loader)
    images, labels = next(data_iter)
    images = images[:num_samples].to(device)
    labels = labels[:num_samples]
    
    # è·å–å„å±‚æ¿€æ´»
    activations = {}
    with torch.no_grad():
        for i, img in enumerate(images):
            img_batch = img.unsqueeze(0)
            if hasattr(model, 'get_activations'):
                acts = model.get_activations(img_batch)
                activations[i] = acts
    
    # åˆ›å»ºåŠ¨ç”»
    fig = plt.figure(figsize=(20, 12))
    gs = GridSpec(3, 4, figure=fig)
    
    def animate(frame):
        # æ¸…é™¤æ‰€æœ‰å­å›¾
        for ax in fig.axes:
            ax.clear()
        
        sample_idx = frame % num_samples
        sample_acts = activations[sample_idx]
        current_label = labels[sample_idx].item()
        
        # åŸå§‹å›¾åƒ
        ax_orig = fig.add_subplot(gs[0, 0])
        img = images[sample_idx].cpu()
        if img.shape[0] == 1:
            ax_orig.imshow(img.squeeze(), cmap='gray')
        else:
            img = img.permute(1, 2, 0)
            ax_orig.imshow(img)
        ax_orig.set_title(f'è¾“å…¥å›¾åƒ\næ ‡ç­¾: {class_names[current_label]}', fontsize=12)
        ax_orig.axis('off')
        
        # Conv1 æ¿€æ´»
        ax_conv1 = fig.add_subplot(gs[0, 1])
        conv1_act = sample_acts['conv1'].squeeze().cpu().numpy()
        conv1_avg = conv1_act.mean(axis=0)
        im1 = ax_conv1.imshow(conv1_avg, cmap='hot')
        ax_conv1.set_title('Conv1 æ¿€æ´»\n(6é€šé“å¹³å‡)', fontsize=12)
        ax_conv1.axis('off')
        
        # Conv2 æ¿€æ´»
        ax_conv2 = fig.add_subplot(gs[0, 2])
        conv2_act = sample_acts['conv2'].squeeze().cpu().numpy()
        conv2_avg = conv2_act.mean(axis=0)
        im2 = ax_conv2.imshow(conv2_avg, cmap='hot')
        ax_conv2.set_title('Conv2 æ¿€æ´»\n(16é€šé“å¹³å‡)', fontsize=12)
        ax_conv2.axis('off')
        
        # è¾“å‡ºæ¦‚ç‡
        ax_output = fig.add_subplot(gs[0, 3])
        output_act = sample_acts['output'].squeeze().cpu().numpy()
        probs = torch.softmax(torch.tensor(output_act), dim=0).numpy()
        bars = ax_output.bar(range(len(class_names)), probs, color='skyblue')
        bars[current_label].set_color('red')
        ax_output.set_title('è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ', fontsize=12)
        ax_output.set_xlabel('ç±»åˆ«')
        ax_output.set_ylabel('æ¦‚ç‡')
        ax_output.set_xticks(range(len(class_names)))
        ax_output.set_xticklabels(class_names, rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, prob in enumerate(probs):
            ax_output.text(i, prob + 0.01, f'{prob:.3f}', ha='center', va='bottom', fontsize=8)
        
        # FC1 æ¿€æ´»
        ax_fc1 = fig.add_subplot(gs[1, :2])
        fc1_act = sample_acts['fc1'].squeeze().cpu().numpy()
        ax_fc1.bar(range(len(fc1_act)), fc1_act, color='lightgreen')
        ax_fc1.set_title('FC1 æ¿€æ´» (120ä¸ªç¥ç»å…ƒ)', fontsize=12)
        ax_fc1.set_xlabel('ç¥ç»å…ƒç´¢å¼•')
        ax_fc1.set_ylabel('æ¿€æ´»å€¼')
        
        # FC2 æ¿€æ´»
        ax_fc2 = fig.add_subplot(gs[1, 2:])
        fc2_act = sample_acts['fc2'].squeeze().cpu().numpy()
        ax_fc2.bar(range(len(fc2_act)), fc2_act, color='orange')
        ax_fc2.set_title('FC2 æ¿€æ´» (84ä¸ªç¥ç»å…ƒ)', fontsize=12)
        ax_fc2.set_xlabel('ç¥ç»å…ƒç´¢å¼•')
        ax_fc2.set_ylabel('æ¿€æ´»å€¼')
        
        # æ¿€æ´»ç»Ÿè®¡ä¿¡æ¯
        ax_stats = fig.add_subplot(gs[2, :])
        ax_stats.axis('off')
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        stats_text = f"""
        æ ·æœ¬ {sample_idx + 1}/{num_samples} | æ ‡ç­¾: {class_names[current_label]}
        
        æ¿€æ´»ç»Ÿè®¡:
        Conv1: å‡å€¼={conv1_act.mean():.4f}, æœ€å¤§å€¼={conv1_act.max():.4f}, æ ‡å‡†å·®={conv1_act.std():.4f}
        Conv2: å‡å€¼={conv2_act.mean():.4f}, æœ€å¤§å€¼={conv2_act.max():.4f}, æ ‡å‡†å·®={conv2_act.std():.4f}
        FC1: å‡å€¼={fc1_act.mean():.4f}, æœ€å¤§å€¼={fc1_act.max():.4f}, æ ‡å‡†å·®={fc1_act.std():.4f}
        FC2: å‡å€¼={fc2_act.mean():.4f}, æœ€å¤§å€¼={fc2_act.max():.4f}, æ ‡å‡†å·®={fc2_act.std():.4f}
        
        é¢„æµ‹: {class_names[np.argmax(probs)]} (ç½®ä¿¡åº¦: {np.max(probs):.4f})
        """
        
        ax_stats.text(0.1, 0.5, stats_text, fontsize=10, verticalalignment='center',
                     bbox=dict(boxstyle="round,pad=0.3", facecolor="lightblue", alpha=0.7))
    
    # åˆ›å»ºåŠ¨ç”»
    anim = animation.FuncAnimation(fig, animate, frames=num_samples*8, 
                                 interval=800, repeat=True)
    
    if save_path:
        anim.save(save_path, writer='pillow', fps=1.5)
    
    plt.tight_layout()
    plt.show()
    
    return anim

def create_feature_evolution_animation(model, test_loader, device, class_names, 
                                     target_digit=7, num_samples=4, save_path=None):
    """åˆ›å»ºç‰¹å¾æ¼”åŒ–åŠ¨ç”»ï¼Œå±•ç¤ºä¸åŒæ ·æœ¬çš„ç‰¹å¾å˜åŒ–"""
    model.eval()
    
    # æ”¶é›†ç›®æ ‡æ•°å­—çš„æ ·æœ¬
    target_samples = []
    target_labels = []
    
    with torch.no_grad():
        for data, labels in test_loader:
            for i, label in enumerate(labels):
                if label.item() == target_digit and len(target_samples) < num_samples:
                    target_samples.append(data[i])
                    target_labels.append(label.item())
            
            if len(target_samples) >= num_samples:
                break
    
    if len(target_samples) == 0:
        print(f"æœªæ‰¾åˆ°æ•°å­— {target_digit} çš„æ ·æœ¬")
        return
    
    # è·å–æ‰€æœ‰æ ·æœ¬çš„æ¿€æ´»
    all_activations = []
    for sample in target_samples:
        sample_batch = sample.unsqueeze(0).to(device)
        if hasattr(model, 'get_activations'):
            acts = model.get_activations(sample_batch)
            all_activations.append(acts)
    
    # åˆ›å»ºåŠ¨ç”»
    fig = plt.figure(figsize=(18, 10))
    gs = GridSpec(2, 4, figure=fig)
    
    def animate(frame):
        # æ¸…é™¤æ‰€æœ‰å­å›¾
        for ax in fig.axes:
            ax.clear()
        
        sample_idx = frame % num_samples
        acts = all_activations[sample_idx]
        sample = target_samples[sample_idx]
        
        # åŸå§‹å›¾åƒ
        ax_orig = fig.add_subplot(gs[0, 0])
        img = sample.squeeze().cpu().numpy()
        ax_orig.imshow(img, cmap='gray')
        ax_orig.set_title(f'æ ·æœ¬ {sample_idx + 1}\næ•°å­— {target_digit}')
        ax_orig.axis('off')
        
        # Conv1 æ¿€æ´»
        ax_conv1 = fig.add_subplot(gs[0, 1])
        conv1_act = acts['conv1'].squeeze().cpu().numpy()
        conv1_avg = conv1_act.mean(axis=0)
        im1 = ax_conv1.imshow(conv1_avg, cmap='hot')
        ax_conv1.set_title('Conv1 æ¿€æ´»')
        ax_conv1.axis('off')
        
        # Conv2 æ¿€æ´»
        ax_conv2 = fig.add_subplot(gs[0, 2])
        conv2_act = acts['conv2'].squeeze().cpu().numpy()
        conv2_avg = conv2_act.mean(axis=0)
        im2 = ax_conv2.imshow(conv2_avg, cmap='hot')
        ax_conv2.set_title('Conv2 æ¿€æ´»')
        ax_conv2.axis('off')
        
        # è¾“å‡ºæ¦‚ç‡
        ax_output = fig.add_subplot(gs[0, 3])
        output_act = acts['output'].squeeze().cpu().numpy()
        probs = torch.softmax(torch.tensor(output_act), dim=0).numpy()
        bars = ax_output.bar(range(len(class_names)), probs, color='skyblue')
        bars[target_digit].set_color('red')
        ax_output.set_title('è¾“å‡ºæ¦‚ç‡')
        ax_output.set_xlabel('ç±»åˆ«')
        ax_output.set_ylabel('æ¦‚ç‡')
        ax_output.set_xticks(range(len(class_names)))
        ax_output.set_xticklabels(class_names, rotation=45)
        
        # ç‰¹å¾å¯¹æ¯”
        ax_compare = fig.add_subplot(gs[1, :])
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„FC2ç‰¹å¾
        all_fc2 = []
        for act in all_activations:
            fc2_act = act['fc2'].squeeze().cpu().numpy()
            all_fc2.append(fc2_act)
        
        # ç»˜åˆ¶ç‰¹å¾å¯¹æ¯”
        x = np.arange(len(all_fc2[0]))
        colors = ['red', 'blue', 'green', 'orange']
        
        for i, fc2 in enumerate(all_fc2):
            ax_compare.plot(x, fc2, color=colors[i], label=f'æ ·æœ¬ {i+1}', alpha=0.7)
        
        ax_compare.set_title(f'æ•°å­— {target_digit} ä¸åŒæ ·æœ¬çš„FC2ç‰¹å¾å¯¹æ¯”')
        ax_compare.set_xlabel('ç‰¹å¾ç»´åº¦')
        ax_compare.set_ylabel('æ¿€æ´»å€¼')
        ax_compare.legend()
        ax_compare.grid(True, alpha=0.3)
        
        # é«˜äº®å½“å‰æ ·æœ¬
        current_fc2 = all_fc2[sample_idx]
        ax_compare.plot(x, current_fc2, color=colors[sample_idx], linewidth=3, alpha=1.0)
    
    # åˆ›å»ºåŠ¨ç”»
    anim = animation.FuncAnimation(fig, animate, frames=num_samples*6, 
                                 interval=1000, repeat=True)
    
    if save_path:
        anim.save(save_path, writer='pillow', fps=1)
    
    plt.tight_layout()
    plt.show()
    
    return anim

def create_comprehensive_visualization(model, test_loader, device, class_names, 
                                     results_dir='./results'):
    """åˆ›å»ºç»¼åˆå¯è§†åŒ–ï¼ŒåŒ…å«æ‰€æœ‰å¢å¼ºåŠŸèƒ½"""
    os.makedirs(results_dir, exist_ok=True)
    
    print("ğŸ¨ å¼€å§‹åˆ›å»ºç»¼åˆå¯è§†åŒ–...")
    
    # 1. å¯è§†åŒ–å·ç§¯æ»¤æ³¢å™¨
    print("1. å¯è§†åŒ–å·ç§¯æ»¤æ³¢å™¨...")
    visualize_conv_filters(model, 'conv1', 
                          os.path.join(results_dir, 'conv1_filters.png'))
    visualize_conv_filters(model, 'conv2', 
                          os.path.join(results_dir, 'conv2_filters.png'))
    
    # 2. å¯è§†åŒ–æ•°å­—7çš„å¤šä¸ªæ ·æœ¬
    print("2. å¯è§†åŒ–æ•°å­—7çš„å¤šä¸ªæ ·æœ¬...")
    visualize_conv_activations_for_digit(model, test_loader, device, target_digit=7,
                                       save_path=os.path.join(results_dir, 'digit7_multiple_samples.png'))
    
    # 3. è¯¦ç»†å¯è§†åŒ–å·ç§¯é€šé“
    print("3. è¯¦ç»†å¯è§†åŒ–å·ç§¯é€šé“...")
    visualize_conv_channels_detailed(model, test_loader, device, target_digit=7,
                                   save_path=os.path.join(results_dir, 'digit7_detailed_channels.png'))
    
    # 4. åˆ›å»ºå¢å¼ºç‰ˆæ¿€æ´»åŠ¨ç”»
    print("4. åˆ›å»ºå¢å¼ºç‰ˆæ¿€æ´»åŠ¨ç”»...")
    anim1 = create_activation_animation_enhanced(model, test_loader, device, class_names,
                                               save_path=os.path.join(results_dir, 'enhanced_activation_animation.gif'))
    
    # 5. åˆ›å»ºç‰¹å¾æ¼”åŒ–åŠ¨ç”»
    print("5. åˆ›å»ºç‰¹å¾æ¼”åŒ–åŠ¨ç”»...")
    anim2 = create_feature_evolution_animation(model, test_loader, device, class_names,
                                             target_digit=7,
                                             save_path=os.path.join(results_dir, 'feature_evolution_animation.gif'))
    
    print(f"âœ… æ‰€æœ‰å¯è§†åŒ–å·²å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ {results_dir}")
    
    return anim1, anim2
